{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Implementing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "df= pd.read_csv('./day5_titanic/train.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['Survived']\n",
    "X=df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### Although a decision tree can handle categorical features and missing data, using the same preprocessing for the sake of comparison with other implemented algorithms. Will compute again to check perforamnce without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeric = X[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
    "X_categorical = X[['Sex','Embarked']]\n",
    "X_neither = X['Cabin']\n",
    "X_onehot = pd.get_dummies(X_categorical,['Sex','Embarked']).astype(int)\n",
    "X_cabin_nonna = X['Cabin'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "X_encoded0 = pd.merge(X_numeric, X_cabin_nonna,  left_index=True, right_index=True)\n",
    "X_encoded1 = pd.merge(X_encoded0, X_onehot,  left_index=True, right_index=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded1, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputer for numeric columns (e.g., Age)\n",
    "imputer = SimpleImputer(strategy='median')  # or 'mean' if you prefer\n",
    "\n",
    "# Fit on train, transform train\n",
    "X_train['Age'] = imputer.fit_transform(X_train[['Age']])\n",
    "\n",
    "# Transform test using same statistics\n",
    "X_test['Age'] = imputer.transform(X_test[['Age']])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Performance on sklearns implementation\n",
    "Hyperparameters: min examples per leaf, max tree depth, min impurity reduction on split\n",
    "Start at X_train_scaled, X_test_scaled, y_train, y_test and apply decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Import libraries\n",
    "# ----------------------------\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ----------------------------\n",
    "# Create and train Decision Tree model\n",
    "# ----------------------------\n",
    "# You can tune parameters like max_depth, min_samples_split, etc.\n",
    "dt_model = DecisionTreeClassifier(random_state=42,max_depth=3)\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# Make predictions\n",
    "# ----------------------------\n",
    "y_train_pred = dt_model.predict(X_train_scaled)\n",
    "y_test_pred = dt_model.predict(X_test_scaled)\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate accuracy\n",
    "# ----------------------------\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Decision Tree Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Decision Tree Test Accuracy:  {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))  # adjust size as needed\n",
    "tree.plot_tree(\n",
    "    dt_model,                 # your trained Decision Tree\n",
    "    feature_names=X_train.columns,  # column names\n",
    "    class_names=['0','1'],    # labels for classes\n",
    "    filled=True,              # color nodes by class\n",
    "    rounded=True,             # rounded boxes\n",
    "    fontsize=12\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    '''\n",
    "    y is a pandas series. return float value of entropy\n",
    "    '''\n",
    "    eps = 1e-9\n",
    "    n_samples = y.shape[0]\n",
    "    \n",
    "    y = np.asarray(y)\n",
    "    cats, counts = np.unique(y, return_counts=True)\n",
    "    pi =  (counts/ n_samples) + eps\n",
    "    pi = pi / sum(pi)\n",
    "    log_pi = np.log2(pi)\n",
    "    entropy = np.dot(pi,log_pi)\n",
    "    return -1 *  entropy\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y = pd.Series(['cat', 'cat', 'dog', 'dog', 'dog'])\n",
    "print(entropy(y))  # Output: ~0.97095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(y, y_l, y_r):\n",
    "    '''\n",
    "    computes infromation gain when y is split into y_l and y_r\n",
    "    '''\n",
    "    y, y_l, y_r = map(np.asarray, (y, y_l, y_r))\n",
    "    n, n_l, n_r = (arr.shape[0] for arr in (y,y_l,y_r))\n",
    "    if n_l == 0 or n_r == 0:\n",
    "        return 0\n",
    "    \n",
    "    term1 = entropy(y)\n",
    "    term2 = (n_l)*entropy(y_l)/(n)\n",
    "    term3 = (n_r)*entropy(y_r)/(n)\n",
    "    return term1 - term2 - term3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.Series(['cat', 'cat', 'dog', 'dog', 'dog'])\n",
    "y_l = pd.Series(['cat', 'cat'])\n",
    "y_r = pd.Series(['dog', 'dog', 'dog'])\n",
    "print(information_gain(y, y_l, y_r))  # Output: ~0.97095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_l = pd.Series(['cat', 'cat','dog'])\n",
    "y_r = pd.Series(['dog', 'dog'])\n",
    "print(information_gain(y, y_l, y_r))  # Output: ~0.97095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_y(y):\n",
    "        y_unique, freq = np.unique(y, return_counts=True)\n",
    "        return y_unique[ np.argmax(freq)]\n",
    "\n",
    "get_most_common_y([1,2,1,1,4,4,4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, depth = 0):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.val = None\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.information_gain = None\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "\n",
    "    \n",
    "\n",
    "class DTree:\n",
    "    def __init__(self, max_depth = 3, min_examples_leaf = 10):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_examples_leaf = min_examples_leaf\n",
    "        self.root = None\n",
    "\n",
    "    def _get_most_common_y(self, y):\n",
    "        y_unique, freq = np.unique(y, return_counts=True)\n",
    "        return y_unique[np.argmax(freq)]\n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if n_samples <= self.min_examples_leaf or len(np.unique(y))==1 or depth>=self.max_depth :\n",
    "            leaf = Node(depth)\n",
    "            leaf.val = self._get_most_common_y(y)\n",
    "            return leaf\n",
    "        else:\n",
    "            \n",
    "            max_inf_gain = -1\n",
    "            feature_idx = None\n",
    "            split_point = None\n",
    "            best_splits = None\n",
    "            for feature_number in range(n_features):\n",
    "                x_iter = X[:,feature_number]\n",
    "                x_iter_sorted = np.sort(x_iter)\n",
    "                for idx, point in enumerate(x_iter_sorted[:-1]):\n",
    "                    midpoint = np.mean([x_iter_sorted[idx],x_iter_sorted[idx+1]]) \n",
    "                    X_lower_indices =  X[:,feature_number]<midpoint\n",
    "                    X_higher_indices = X[:,feature_number]>=midpoint\n",
    "                    inf_gain = information_gain(y,y[X_lower_indices],y[X_higher_indices])\n",
    "                    if inf_gain > max_inf_gain:\n",
    "                        max_inf_gain = inf_gain\n",
    "                        split_point = midpoint\n",
    "                        feature_idx = feature_number\n",
    "                        best_splits = (X_lower_indices, X_higher_indices)\n",
    "            if max_inf_gain <=0 or len(y[X_lower_indices]) == 0 or len(y[X_higher_indices]) == 0: ## Should the equality be here, yes, if info gain is 0 there is no point in splitting \n",
    "            ### need the len checks above since previous features have turned the max_inf_gain to be positive \n",
    "                leaf = Node(depth)\n",
    "                leaf.val = self._get_most_common_y(y)\n",
    "                return leaf\n",
    "            else:\n",
    "                node = Node(depth)\n",
    "                node.left = self.fit(X[X_lower_indices],y[X_lower_indices],depth+1)\n",
    "                node.right = self.fit(X[X_higher_indices],y[X_higher_indices],depth+1)\n",
    "                \n",
    "                self.information_gain = max_inf_gain\n",
    "                self.feature_index = feature_idx\n",
    "                self.threshold = split_point\n",
    "                return node\n",
    "                \n",
    "\n",
    "\n",
    "    def predict_one_example(self, x, node=None):\n",
    "        '''\n",
    "        x is a list of n_features * 1, why is node passed when we already have self. node is an object of a different class. \n",
    "        '''\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        if node.val is not None: ## leaf node\n",
    "            return node.val\n",
    "        feature_idx = node.threshold\n",
    "        thresh = node.feature_index\n",
    "        print(x[feature_idx], thresh)\n",
    "        if x[feature_idx] < thresh:\n",
    "            return self.predict_one_example(x,node.left)\n",
    "        else:\n",
    "            return self.predict_one_example(x,node.right)\n",
    "        \n",
    "    def predict_all(self, X):\n",
    "        '''\n",
    "        y is a list of n_examples dimension\n",
    "        '''\n",
    "        y = [self.predict_one_example(x) for x in X]\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Public API for fitting\"\"\"\n",
    "        self.root = self.fit(X, y, 0)     \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Creating a new PR to test DTree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DTree()\n",
    "a.train(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = a.predict_one_example(X_train_scaled[0,:])\n",
    "type(y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[3,4,5,6,7,8],[1,2,3,4,5,6]])\n",
    "\n",
    "def predict_per_row(row):\n",
    "    return np.max(row)\n",
    "\n",
    "def predict_all(X):\n",
    "    '''\n",
    "        y is a list of n_examples dimension\n",
    "    '''\n",
    "    y = [predict_per_row(x) for x in X]\n",
    "    return (y)\n",
    "\n",
    "\n",
    "\n",
    "print(predict_all(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### Q1. How is a split evaluated in the code, we do a split, then what ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Import libraries\n",
    "# ----------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ----------------------------\n",
    "# Create and train Random Forest model\n",
    "# ----------------------------\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,      # number of trees\n",
    "    max_depth=3,           # depth of each tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# Make predictions\n",
    "# ----------------------------\n",
    "y_train_pred = rf_model.predict(X_train_scaled)\n",
    "y_test_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate accuracy\n",
    "# ----------------------------\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Random Forest Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Random Forest Test Accuracy:  {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Base estimator: shallow decision tree\n",
    "base_dt = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "# Create AdaBoost model (new parameter name)\n",
    "ada_model = AdaBoostClassifier(\n",
    "    estimator=base_dt,       # <-- use 'estimator' instead of 'base_estimator'\n",
    "    n_estimators=29,\n",
    "    learning_rate=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "ada_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_train_pred = ada_model.predict(X_train_scaled)\n",
    "y_test_pred = ada_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"AdaBoost Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"AdaBoost Test Accuracy:  {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
