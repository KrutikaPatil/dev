{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Implementing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f04cdd7-c4d7-45cb-9f5f-965b11fdfd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "\n",
       "   Parch     Ticket     Fare Cabin Embarked  \n",
       "0      0  A/5 21171   7.2500   NaN        S  \n",
       "1      0   PC 17599  71.2833   C85        C  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "df= pd.read_csv('./day5_titanic/train.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fe1439e-7f35-4fe4-a18c-d18f0e0d3618",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['Survived']\n",
    "X=df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### Although a decision tree can handle categorical features and missing data, using the same preprocessing for the sake of comparison with other implemented algorithms. Will compute again to check perforamnce without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33076e0c-3f47-4dff-ab96-68ccc0391432",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeric = X[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
    "X_categorical = X[['Sex','Embarked']]\n",
    "X_neither = X['Cabin']\n",
    "X_onehot = pd.get_dummies(X_categorical,['Sex','Embarked']).astype(int)\n",
    "X_cabin_nonna = X['Cabin'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "X_encoded0 = pd.merge(X_numeric, X_cabin_nonna,  left_index=True, right_index=True)\n",
    "X_encoded1 = pd.merge(X_encoded0, X_onehot,  left_index=True, right_index=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded1, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputer for numeric columns (e.g., Age)\n",
    "imputer = SimpleImputer(strategy='median')  # or 'mean' if you prefer\n",
    "\n",
    "# Fit on train, transform train\n",
    "X_train['Age'] = imputer.fit_transform(X_train[['Age']])\n",
    "\n",
    "# Transform test using same statistics\n",
    "X_test['Age'] = imputer.transform(X_test[['Age']])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Performance on sklearns implementation\n",
    "Hyperparameters: min examples per leaf, max tree depth, min impurity reduction on split\n",
    "Start at X_train_scaled, X_test_scaled, y_train, y_test and apply decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8855fa1f-2d46-43cf-a8c9-6235acc37d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Import libraries\n",
    "# ----------------------------\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ----------------------------\n",
    "# Create and train Decision Tree model\n",
    "# ----------------------------\n",
    "# You can tune parameters like max_depth, min_samples_split, etc.\n",
    "dt_model = DecisionTreeClassifier(random_state=42,max_depth=3)\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# Make predictions\n",
    "# ----------------------------\n",
    "y_train_pred = dt_model.predict(X_train_scaled)\n",
    "y_test_pred = dt_model.predict(X_test_scaled)\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate accuracy\n",
    "# ----------------------------\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Decision Tree Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Decision Tree Test Accuracy:  {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))  # adjust size as needed\n",
    "tree.plot_tree(\n",
    "    dt_model,                 # your trained Decision Tree\n",
    "    feature_names=X_train.columns,  # column names\n",
    "    class_names=['0','1'],    # labels for classes\n",
    "    filled=True,              # color nodes by class\n",
    "    rounded=True,             # rounded boxes\n",
    "    fontsize=12\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1aed5a1-f0cc-4464-a511-26748726b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    '''\n",
    "    y is a pandas series. return float value of entropy\n",
    "    '''\n",
    "    eps = 1e-9\n",
    "    n_samples = y.shape[0]\n",
    "    \n",
    "    y = np.asarray(y)\n",
    "    cats, counts = np.unique(y, return_counts=True)\n",
    "    pi =  (counts/ n_samples) + eps\n",
    "    pi = pi / sum(pi)\n",
    "    log_pi = np.log2(pi)\n",
    "    entropy = np.dot(pi,log_pi)\n",
    "    return -1 *  entropy\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y = pd.Series(['cat', 'cat', 'dog', 'dog', 'dog'])\n",
    "print(entropy(y))  # Output: ~0.97095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(y, y_l, y_r):\n",
    "    '''\n",
    "    computes infromation gain when y is split into y_l and y_r\n",
    "    '''\n",
    "    y, y_l, y_r = map(np.asarray, (y, y_l, y_r))\n",
    "    n, n_l, n_r = (arr.shape[0] for arr in (y,y_l,y_r))\n",
    "    if n_l == 0 or n_r == 0:\n",
    "        return 0\n",
    "    \n",
    "    term1 = entropy(y)\n",
    "    term2 = (n_l)*entropy(y_l)/(n)\n",
    "    term3 = (n_r)*entropy(y_r)/(n)\n",
    "    return term1 - term2 - term3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.Series(['cat', 'cat', 'dog', 'dog', 'dog'])\n",
    "y_l = pd.Series(['cat', 'cat'])\n",
    "y_r = pd.Series(['dog', 'dog', 'dog'])\n",
    "print(information_gain(y, y_l, y_r))  # Output: ~0.97095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_l = pd.Series(['cat', 'cat','dog'])\n",
    "y_r = pd.Series(['dog', 'dog'])\n",
    "print(information_gain(y, y_l, y_r))  # Output: ~0.97095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d693431-800e-4993-bd7b-ec36e6cc46f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_most_common_y(y):\n",
    "        y_unique, freq = np.unique(y, return_counts=True)\n",
    "        return y_unique[ np.argmax(freq)]\n",
    "\n",
    "get_most_common_y([1,2,1,1,4,4,4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, depth = 0):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.val = None\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.information_gain = None\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "\n",
    "    \n",
    "\n",
    "class DTree:\n",
    "    def __init__(self, max_depth = 3, min_examples_leaf = 10):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_examples_leaf = min_examples_leaf\n",
    "        self.root = None\n",
    "\n",
    "    def _get_most_common_y(self, y):\n",
    "        y_unique, freq = np.unique(y, return_counts=True)\n",
    "        return y_unique[np.argmax(freq)]\n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if n_samples <= self.min_examples_leaf or len(np.unique(y))==1 or depth>=self.max_depth :\n",
    "            leaf = Node(depth)\n",
    "            leaf.val = self._get_most_common_y(y)\n",
    "            return leaf\n",
    "        else:\n",
    "            \n",
    "            max_inf_gain = -1\n",
    "            feature_idx = None\n",
    "            split_point = None\n",
    "            best_splits = None\n",
    "            for feature_number in range(n_features):\n",
    "                x_iter = X[:,feature_number]\n",
    "                x_iter_sorted = np.sort(x_iter)\n",
    "                for idx, point in enumerate(x_iter_sorted[:-1]):\n",
    "                    midpoint = np.mean([x_iter_sorted[idx],x_iter_sorted[idx+1]]) \n",
    "                    X_lower_indices =  X[:,feature_number]<midpoint\n",
    "                    X_higher_indices = X[:,feature_number]>=midpoint\n",
    "                    inf_gain = information_gain(y,y[X_lower_indices],y[X_higher_indices])\n",
    "                    if inf_gain > max_inf_gain:\n",
    "                        max_inf_gain = inf_gain\n",
    "                        split_point = midpoint\n",
    "                        feature_idx = feature_number\n",
    "                        best_splits = (X_lower_indices, X_higher_indices)\n",
    "            if max_inf_gain <=0 or len(y[X_lower_indices]) == 0 or len(y[X_higher_indices]) == 0: ## Should the equality be here, yes, if info gain is 0 there is no point in splitting \n",
    "            ### need the len checks above since previous features have turned the max_inf_gain to be positive \n",
    "                leaf = Node(depth)\n",
    "                leaf.val = self._get_most_common_y(y)\n",
    "                return leaf\n",
    "            else:\n",
    "                node = Node(depth)\n",
    "                node.left = self.fit(X[X_lower_indices],y[X_lower_indices],depth+1)\n",
    "                node.right = self.fit(X[X_higher_indices],y[X_higher_indices],depth+1)\n",
    "                \n",
    "                node.information_gain = max_inf_gain\n",
    "                node.feature_index = feature_idx\n",
    "                node.threshold = split_point\n",
    "                return node\n",
    "                \n",
    "\n",
    "\n",
    "    def predict_one_example(self, x, node=None):\n",
    "        '''\n",
    "        x is a list of n_features * 1, why is node passed when we already have self. node is an object of a different class. \n",
    "        '''\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        if node.val is not None: ## leaf node\n",
    "            return node.val\n",
    "        thresh = node.threshold\n",
    "        feature_idx = node.feature_index\n",
    "        #print(x[feature_idx], thresh)\n",
    "        if x[feature_idx] < thresh:\n",
    "            return self.predict_one_example(x,node.left)\n",
    "        else:\n",
    "            return self.predict_one_example(x,node.right)\n",
    "        \n",
    "    def predict_all(self, X):\n",
    "        '''\n",
    "        y is a list of n_examples dimension\n",
    "        '''\n",
    "        y = [self.predict_one_example(x) for x in X]\n",
    "        return y\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Public API for fitting\"\"\"\n",
    "        self.root = self.fit(X, y, 0)     \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Creating a new PR to test DTree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DTree()\n",
    "a.train(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = a.predict_one_example(X_train_scaled[0,:])\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = a.predict_all(X_train_scaled)\n",
    "type(y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute classification accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true: pandas Series or array-like\n",
    "        y_pred: list or array-like\n",
    "    Returns:\n",
    "        float: accuracy score (between 0 and 1)\n",
    "    \"\"\"\n",
    "    # Convert both inputs to NumPy arrays\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    # Check they have the same length\n",
    "    if y_true.shape[0] != y_pred.shape[0]:\n",
    "        raise ValueError(\"y_true and y_pred must have the same length.\")\n",
    "    \n",
    "    # Compute accuracy\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    total = len(y_true)\n",
    "    return correct / total\n",
    "\n",
    "print(accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = a.predict_all(X_test_scaled)\n",
    "print(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_get_most_common_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m DTree()\n\u001b[0;32m----> 2\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 89\u001b[0m, in \u001b[0;36mDTree.train\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Public API for fitting\"\"\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 56\u001b[0m, in \u001b[0;36mDTree.fit\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     node \u001b[38;5;241m=\u001b[39m Node(depth)\n\u001b[0;32m---> 56\u001b[0m     node\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_lower_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_lower_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     node\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X[X_higher_indices],y[X_higher_indices],depth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minformation_gain \u001b[38;5;241m=\u001b[39m max_inf_gain\n",
      "Cell \u001b[0;32mIn[25], line 56\u001b[0m, in \u001b[0;36mDTree.fit\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     node \u001b[38;5;241m=\u001b[39m Node(depth)\n\u001b[0;32m---> 56\u001b[0m     node\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_lower_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_lower_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     node\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X[X_higher_indices],y[X_higher_indices],depth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minformation_gain \u001b[38;5;241m=\u001b[39m max_inf_gain\n",
      "Cell \u001b[0;32mIn[25], line 29\u001b[0m, in \u001b[0;36mDTree.fit\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_examples_leaf \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y))\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m depth\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth :\n\u001b[1;32m     28\u001b[0m     leaf \u001b[38;5;241m=\u001b[39m Node(depth)\n\u001b[0;32m---> 29\u001b[0m     leaf\u001b[38;5;241m.\u001b[39mval \u001b[38;5;241m=\u001b[39m \u001b[43m_get_most_common_y\u001b[49m(y)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m leaf\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name '_get_most_common_y' is not defined"
     ]
    }
   ],
   "source": [
    "a = DTree()\n",
    "a.train(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36fe0a-574e-4f06-8eba-e8598439891a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[3,4,5,6,7,8],[1,2,3,4,5,6]])\n",
    "\n",
    "def predict_per_row(row):\n",
    "    return np.max(row)\n",
    "\n",
    "def predict_all(X):\n",
    "    '''\n",
    "        y is a list of n_examples dimension\n",
    "    '''\n",
    "    y = [predict_per_row(x) for x in X]\n",
    "    return (y)\n",
    "\n",
    "\n",
    "\n",
    "print(predict_all(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[3,4,5,6,7,8],[1,2,3,4,5,6]])\n",
    "\n",
    "def predict_per_row(row):\n",
    "    return np.max(row)\n",
    "\n",
    "def predict_all(X):\n",
    "    '''\n",
    "        y is a list of n_examples dimension\n",
    "    '''\n",
    "    y = [predict_per_row(x) for x in X]\n",
    "    return (y)\n",
    "\n",
    "\n",
    "\n",
    "print(predict_all(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Q1. How is a split evaluated in the code, we do a split, then what ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Import libraries\n",
    "# ----------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ----------------------------\n",
    "# Create and train Random Forest model\n",
    "# ----------------------------\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,      # number of trees\n",
    "    max_depth=3,           # depth of each tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# Make predictions\n",
    "# ----------------------------\n",
    "y_train_pred = rf_model.predict(X_train_scaled)\n",
    "y_test_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate accuracy\n",
    "# ----------------------------\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Random Forest Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Random Forest Test Accuracy:  {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Base estimator: shallow decision tree\n",
    "base_dt = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "# Create AdaBoost model (new parameter name)\n",
    "ada_model = AdaBoostClassifier(\n",
    "    estimator=base_dt,       # <-- use 'estimator' instead of 'base_estimator'\n",
    "    n_estimators=29,\n",
    "    learning_rate=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "ada_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_train_pred = ada_model.predict(X_train_scaled)\n",
    "y_test_pred = ada_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"AdaBoost Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"AdaBoost Test Accuracy:  {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
