{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3d2c39-6297-4adf-b9aa-f363c4ff62dc",
   "metadata": {},
   "source": [
    "## Implementing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04cdd7-c4d7-45cb-9f5f-965b11fdfd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "df= pd.read_csv('./day5_titanic/train.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe1439e-7f35-4fe4-a18c-d18f0e0d3618",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['Survived']\n",
    "X=df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c462939b-d8c8-4f95-9dc1-493218997884",
   "metadata": {},
   "source": [
    "#### Although a decision tree can handle categorical features and missing data, using the same preprocessing for the sake of comparison with other implemented algorithms. Will compute again to check perforamnce without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33076e0c-3f47-4dff-ab96-68ccc0391432",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeric = X[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
    "X_categorical = X[['Sex','Embarked']]\n",
    "X_neither = X['Cabin']\n",
    "X_onehot = pd.get_dummies(X_categorical,['Sex','Embarked']).astype(int)\n",
    "X_cabin_nonna = X['Cabin'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "X_encoded0 = pd.merge(X_numeric, X_cabin_nonna,  left_index=True, right_index=True)\n",
    "X_encoded1 = pd.merge(X_encoded0, X_onehot,  left_index=True, right_index=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded1, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputer for numeric columns (e.g., Age)\n",
    "imputer = SimpleImputer(strategy='median')  # or 'mean' if you prefer\n",
    "\n",
    "# Fit on train, transform train\n",
    "X_train['Age'] = imputer.fit_transform(X_train[['Age']])\n",
    "\n",
    "# Transform test using same statistics\n",
    "X_test['Age'] = imputer.transform(X_test[['Age']])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17285b39-4689-486c-b4ca-cf241878e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a975962-f2a7-404d-aae4-9d1feb3a1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(X_train['Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303a1bf-ae07-4b5e-a36c-16b6cb72d368",
   "metadata": {},
   "source": [
    "### Performance on sklearns implementation\n",
    "Hyperparameters: min examples per leaf, max tree depth, min impurity reduction on split\n",
    "Start at X_train_scaled, X_test_scaled, y_train, y_test and apply decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855fa1f-2d46-43cf-a8c9-6235acc37d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Import libraries\n",
    "# ----------------------------\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ----------------------------\n",
    "# Create and train Decision Tree model\n",
    "# ----------------------------\n",
    "# You can tune parameters like max_depth, min_samples_split, etc.\n",
    "dt_model = DecisionTreeClassifier(random_state=42,max_depth=3)\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# Make predictions\n",
    "# ----------------------------\n",
    "y_train_pred = dt_model.predict(X_train_scaled)\n",
    "y_test_pred = dt_model.predict(X_test_scaled)\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate accuracy\n",
    "# ----------------------------\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Decision Tree Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Decision Tree Test Accuracy:  {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49533a0f-b72a-4e20-b24b-1a642c5d6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))  # adjust size as needed\n",
    "tree.plot_tree(\n",
    "    dt_model,                 # your trained Decision Tree\n",
    "    feature_names=X_train.columns,  # column names\n",
    "    class_names=['0','1'],    # labels for classes\n",
    "    filled=True,              # color nodes by class\n",
    "    rounded=True,             # rounded boxes\n",
    "    fontsize=12\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479ea02-b15a-4c97-856e-ca19a353ca6d",
   "metadata": {},
   "source": [
    "### Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aed5a1-f0cc-4464-a511-26748726b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your entropy function\n",
    "def entropy(y):\n",
    "    '''\n",
    "    y is a pandas series. return float value of entropy\n",
    "    '''\n",
    "    eps = 1e-9\n",
    "    n_samples = y.shape[0]\n",
    "    \n",
    "    y = np.asarray(y)\n",
    "    cats, counts = np.unique(y, return_counts=True)\n",
    "    pi =  (counts/ n_samples) + eps\n",
    "    pi = pi / sum(pi)\n",
    "    log_pi = np.log2(pi)\n",
    "    entropy = np.dot(pi,log_pi)\n",
    "    return -1 *  entropy\n",
    "\n",
    "# ------------------------------\n",
    "# Test cases\n",
    "# ------------------------------\n",
    "\n",
    "# 1. Simple binary classification\n",
    "y1 = pd.Series(['cat', 'cat', 'dog', 'dog', 'dog'])\n",
    "expected1 = -((2/5)*np.log2(2/5) + (3/5)*np.log2(3/5))\n",
    "assert abs(entropy(y1) - expected1) < 1e-6, \"Test 1 failed\"\n",
    "print(\"Test 1 passed\")\n",
    "\n",
    "# 2. Uniform distribution\n",
    "y2 = pd.Series(['a', 'b', 'c', 'd'])\n",
    "expected2 = -sum([0.25*np.log2(0.25)]*4)  # log2(0.25) = -2\n",
    "assert abs(entropy(y2) - expected2) < 1e-6, \"Test 2 failed\"\n",
    "print(\"Test 2 passed\")\n",
    "\n",
    "# 3. Single category (entropy should be 0)\n",
    "y3 = pd.Series(['only', 'only', 'only'])\n",
    "expected3 = 0\n",
    "assert abs(entropy(y3) - expected3) < 1e-6, \"Test 3 failed\"\n",
    "print(\"Test 3 passed\")\n",
    "\n",
    "# 4. Numerical labels\n",
    "y4 = pd.Series([1, 1, 1, 2])\n",
    "expected4 = -((3/4)*np.log2(3/4) + (1/4)*np.log2(1/4))\n",
    "assert abs(entropy(y4) - expected4) < 1e-6, \"Test 4 failed\"\n",
    "print(\"Test 4 passed\")\n",
    "\n",
    "# 5. Empty series (entropy should handle gracefully)\n",
    "y5 = pd.Series([])\n",
    "try:\n",
    "    e5 = entropy(y5)\n",
    "    print(\"Test 5 output:\", e5)\n",
    "except Exception as ex:\n",
    "    print(\"Test 5 passed (handled empty input with exception):\", ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a279d-1ef0-46f7-9968-e5e15637ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y = pd.Series(['cat', 'cat', 'dog', 'dog', 'dog'])\n",
    "print(entropy(y))  # Output: ~0.97095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4cd4a-ab6a-40f2-8ca7-ccfdff4adfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(y, y_l, y_r):\n",
    "    '''\n",
    "    computes infromation gain when y is split into y_l and y_r\n",
    "    '''\n",
    "    y, y_l, y_r = map(np.asarray, (y, y_l, y_r))\n",
    "    n, n_l, n_r = (arr.shape[0] for arr in (y,y_l,y_r))\n",
    "    if n_l == 0 or n_r == 0:\n",
    "        return 0\n",
    "    \n",
    "    term1 = entropy(y)\n",
    "    term2 = (n_l)*entropy(y_l)/(n)\n",
    "    term3 = (n_r)*entropy(y_r)/(n)\n",
    "    return term1 - term2 - term3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3338dd4a-f057-47b1-9d10-e9dd00c15553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Test cases\n",
    "# ------------------------------\n",
    "\n",
    "# 1. Perfect split (all left are 'cat', all right are 'dog')\n",
    "y = pd.Series(['cat','cat','dog','dog'])\n",
    "y_l = pd.Series(['cat','cat'])\n",
    "y_r = pd.Series(['dog','dog'])\n",
    "ig = information_gain(y, y_l, y_r)\n",
    "expected = entropy(y)  # Perfect split should reduce weighted entropies to 0\n",
    "assert abs(ig - expected) < 1e-6, \"Test 1 failed\"\n",
    "print(\"Test 1 passed\")\n",
    "\n",
    "# 2. Split with mixed classes (partial gain)\n",
    "y = pd.Series(['cat','cat','dog','dog'])\n",
    "y_l = pd.Series(['cat','dog'])\n",
    "y_r = pd.Series(['cat','dog'])\n",
    "ig = information_gain(y, y_l, y_r)\n",
    "assert 0 <= ig < entropy(y), \"Test 2 failed\"  # gain should be positive but less than original entropy\n",
    "print(\"Test 2 passed\")\n",
    "\n",
    "# 3. One empty side (should return 0)\n",
    "y = pd.Series(['cat','dog'])\n",
    "y_l = pd.Series([])\n",
    "y_r = pd.Series(['cat','dog'])\n",
    "ig = information_gain(y, y_l, y_r)\n",
    "assert ig == 0, \"Test 3 failed\"\n",
    "print(\"Test 3 passed\")\n",
    "\n",
    "# 4. Single element splits\n",
    "y = pd.Series(['cat','dog'])\n",
    "y_l = pd.Series(['cat'])\n",
    "y_r = pd.Series(['dog'])\n",
    "ig = information_gain(y, y_l, y_r)\n",
    "expected = entropy(y)  # perfect split of single elements should yield full entropy\n",
    "assert abs(ig - expected) < 1e-6, \"Test 4 failed\"\n",
    "print(\"Test 4 passed\")\n",
    "\n",
    "# 5. Numeric labels\n",
    "y = pd.Series([1,1,1,2])\n",
    "y_l = pd.Series([1,1])\n",
    "y_r = pd.Series([1,2])\n",
    "ig = information_gain(y, y_l, y_r)\n",
    "assert 0 <= ig <= entropy(y), \"Test 5 failed\"\n",
    "print(\"Test 5 passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1788c8-0b24-43f1-b36b-89d2f0ee3809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, depth = 0):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.val = None\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.information_gain = None\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return  f\"val:{self.val}\" + f\"depth:{self.depth}\" + \\\n",
    "                f\"information_gain:{self.information_gain}\" + \\\n",
    "                f\"feature_index:{self.feature_index}\" + \\\n",
    "                f\"threshold:{self.threshold}\"\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "class DTree:\n",
    "    def __init__(self, max_depth = 3, min_examples_leaf = 10):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_examples_leaf = min_examples_leaf\n",
    "        self.root = None\n",
    "\n",
    "    def _get_most_common_y(self, y):\n",
    "        y_unique, freq = np.unique(y, return_counts=True)\n",
    "        return y_unique[np.argmax(freq)]\n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        print(f\" n_samples {n_samples} len(np.unique(y)) {len(np.unique(y))} depth {depth}\")\n",
    "        if n_samples <= self.min_examples_leaf or len(np.unique(y))==1 or depth>=self.max_depth :\n",
    "            leaf = Node(depth)\n",
    "            \n",
    "            leaf.val = self._get_most_common_y(y)\n",
    "            \n",
    "            return leaf\n",
    "        else:\n",
    "            \n",
    "            max_inf_gain = -np.inf\n",
    "            feature_idx = None\n",
    "            split_point = None\n",
    "            best_splits = None\n",
    "            for feature_number in range(n_features):\n",
    "                x_iter = X[:,feature_number]\n",
    "                x_iter_sorted = np.sort(x_iter)\n",
    "                for idx, point in enumerate(x_iter_sorted[:-1]):\n",
    "                    midpoint = np.mean([x_iter_sorted[idx],x_iter_sorted[idx+1]]) \n",
    "                    X_lower_indices =  X[:,feature_number]<midpoint\n",
    "                    X_higher_indices = X[:,feature_number]>=midpoint\n",
    "                    inf_gain = information_gain(y, y[X_lower_indices], y[X_higher_indices])\n",
    "                    if len(y[X_lower_indices]) * len(y[X_higher_indices]) ==0 and (inf_gain!=0): \n",
    "                        print(f\" len lower indices after ig {len(y[X_lower_indices])}, higher {len(y[X_higher_indices])} inf_gain {inf_gain}\")\n",
    "                    \n",
    "                    if inf_gain > max_inf_gain:\n",
    "                        max_inf_gain = inf_gain\n",
    "                        split_point = midpoint\n",
    "                        feature_idx = feature_number\n",
    "                        best_splits = (X_lower_indices, X_higher_indices)\n",
    "            \n",
    "            print(f\"max_inf_gain {max_inf_gain} len(y[X_lower_indices]) {len(y[X_lower_indices])} len(y[X_higher_indices]) {len(y[X_higher_indices])}\")\n",
    "            if max_inf_gain <=0:  \n",
    "                #len(y[X_lower_indices]) == 0 or \\\n",
    "                #len(y[X_higher_indices]) == 0: \n",
    "                \n",
    "                # Should the equality be here, yes, if info gain is 0 there is no point in splitting \n",
    "                # need the len checks above since previous features have turned the max_inf_gain to be positive \n",
    "                # dont need the length checks since max_inf_gain would be 0 if either of lleft or right children have 0 elements \n",
    "                leaf = Node(depth)\n",
    "                leaf.val = self._get_most_common_y(y)\n",
    "                return leaf\n",
    "            else:\n",
    "                node = Node(depth)\n",
    "                \n",
    "                node.left = self.fit(X[best_splits[0],:],y[best_splits[0]],depth+1)\n",
    "                node.right = self.fit(X[best_splits[1],:],y[best_splits[1]],depth+1)\n",
    "                \n",
    "                node.information_gain = max_inf_gain\n",
    "                node.feature_index = feature_idx\n",
    "                node.threshold = split_point\n",
    "                return node\n",
    "                \n",
    "\n",
    "\n",
    "    def predict_one_example(self, x, node=None):\n",
    "        '''\n",
    "        x is a list of n_features * 1, why is node passed when we already have self. node is an object of a different class. \n",
    "        '''\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        if node.val is not None: ## leaf node\n",
    "            return node.val\n",
    "        thresh = node.threshold\n",
    "        feature_idx = node.feature_index\n",
    "        #print(f\"feature_idx {feature_idx} x[feature idx] {x[feature_idx]}, thresh {thresh} node.val {node.val} \")\n",
    "        if x[feature_idx] < thresh:\n",
    "            return self.predict_one_example(x,node.left)\n",
    "        else:\n",
    "            return self.predict_one_example(x,node.right)\n",
    "        \n",
    "    def predict_all(self, X):\n",
    "        '''\n",
    "        y is a list of n_examples dimension\n",
    "        '''\n",
    "        y = [self.predict_one_example(x) for x in X]\n",
    "        return y\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Public API for fitting\"\"\"\n",
    "        self.root = self.fit(X, y, 0)     \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Creating a new PR to test DTree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DTree(max_depth=3)\n",
    "a.train(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c366f-8aef-4c0b-a17a-8394889325e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b935bb-3a52-44ac-a1cd-56b7abfcf814",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.root.left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2ffdc-c791-48bc-822f-19ae665d6d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.root.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d77b2-0181-4f94-a0e7-c8cc351e4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d739e4-8a39-44b5-ac43-ec53f85c5fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled[0,:][None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = a.predict_one_example(X_train_scaled[0,:])\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e32aa8-73d4-406a-83ad-80983b6ea848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_test_pred = a.predict_all(X_test_scaled)\n",
    "#y_test = y_test.tolist()\n",
    "print(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c0606-dd94-4ad4-aba6-8ea3f73f8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = a.predict_all(X_train_scaled)\n",
    "#y_test = y_test.tolist()\n",
    "print(accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating a new PR to test DTree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ab2b4-153d-444a-aace-4d509d1d93cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DTree()\n",
    "a.train(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36fe0a-574e-4f06-8eba-e8598439891a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[3,4,5,6,7,8],[1,2,3,4,5,6]])\n",
    "\n",
    "def predict_per_row(row):\n",
    "    return np.max(row)\n",
    "\n",
    "def predict_all(X):\n",
    "    '''\n",
    "        y is a list of n_examples dimension\n",
    "    '''\n",
    "    y = [predict_per_row(x) for x in X]\n",
    "    return (y)\n",
    "\n",
    "\n",
    "\n",
    "print(predict_all(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[3,4,5,6,7,8],[1,2,3,4,5,6]])\n",
    "\n",
    "def predict_per_row(row):\n",
    "    return np.max(row)\n",
    "\n",
    "def predict_all(X):\n",
    "    '''\n",
    "        y is a list of n_examples dimension\n",
    "    '''\n",
    "    y = [predict_per_row(x) for x in X]\n",
    "    return (y)\n",
    "\n",
    "\n",
    "\n",
    "print(predict_all(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cefc3c-0e37-485e-9a5e-c9f10c8606a1",
   "metadata": {},
   "source": [
    "#### Q1. How is a split evaluated in the code, we do a split, then what ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9359a-88fa-4922-87c3-c43087b8b7a2",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff6b0fa-26d5-4803-9490-a2159be7e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Import libraries\n",
    "# ----------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ----------------------------\n",
    "# Create and train Random Forest model\n",
    "# ----------------------------\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,      # number of trees\n",
    "    max_depth=3,           # depth of each tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# Make predictions\n",
    "# ----------------------------\n",
    "y_train_pred = rf_model.predict(X_train_scaled)\n",
    "y_test_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate accuracy\n",
    "# ----------------------------\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Random Forest Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Random Forest Test Accuracy:  {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc01ade-6b5c-41f7-a65b-cfa7522fb555",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e2ab0-995c-4b0a-93ad-39a939c762de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Base estimator: shallow decision tree\n",
    "base_dt = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "# Create AdaBoost model (new parameter name)\n",
    "ada_model = AdaBoostClassifier(\n",
    "    estimator=base_dt,       # <-- use 'estimator' instead of 'base_estimator'\n",
    "    n_estimators=29,\n",
    "    learning_rate=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "ada_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_train_pred = ada_model.predict(X_train_scaled)\n",
    "y_test_pred = ada_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"AdaBoost Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"AdaBoost Test Accuracy:  {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08a575-80dc-4932-86d1-3b7a7ff427df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
